---
title: tensorflow概念整理
comments: true
copyright: true
tags:
  - AI
  - tensorflow
categories:
  - tensorflow
abbrlink: e5f5d28
date: 2022-08-29 20:52:29
---

Tensorflow使用学习说明:

1,环境配置(Window环境下):

[软件链接地址](https://support.microsoft.com/zh-cn/topic/%E6%9C%80%E6%96%B0%E6%94%AF%E6%8C%81%E7%9A%84-visual-c-%E4%B8%8B%E8%BD%BD-2647da03-1eea-4433-9aff-95f26a218cc0)

安装VC_redist.x64.exe软件,Microsoft Visual C++ 环境

2,安装tensorflow,

版本说明:

针对电脑CUDA大小下载不同版本的tensorflow

[对应关系](https://tensorflow.google.cn/install/source_windows)

[查看CUDA方法](https://blog.csdn.net/qq_38295511/article/details/89223169)

①打开控制面板

②搜索NVIDIA控制面板

③点击系统信息

④点击组件

⑤通过NVCUDA.DLL查看CUDA版本

eg:本机的CUDA版本为11.3.55 , 对应的tensorflow版本为2.4.0

下载方式为:https://pypi.org/project/tensorflow/2.4.0/#files

pip install 包名

线性回归和梯度下降算法

单变量线性回归算法: f(x) = ax + b,映射输入特征和输出值

#绘制点状图,受教育年限和收入之间的线性关系: f(x) = ax + b 

预测目标与损失函数

目标:

	预测函数f(x)与真实值之间的整体误差最小
	
	如何定义误差最小

损失函数:

	使用均方差作为成本函数,
	
	均方差:
	
		预测值和真实值之间差的平方取均值

成本函数与损失函数

优化目标(y代表实际的收入)

找到合适的a和b,使(f(x)-y)²越小越好  --->成本函数(均方差)    [f(x) = ax + b]    --->  ((ax + b) - y)²值最小

已知:x和y

注意: 求解的是参数a和b

成本函数与损失函数

如何优化:

使用梯度下降算法

梯度下降算法:

	是一种致力于找到函数极值点的算法,将梯度下降算法应用于寻找损失函数的极值点便构成了依据输入数据的模型学习
	
	梯度的输出是一个由若干偏导数构成的向量,它的每个分量对应于函数对输入向量的相应分量的偏导  (查找之前学过的求偏导)


	梯度的输出向量表明了在每个位置损失函数增长最快的方向,可将它视为表示了在函数的每个位置向哪个方向移动函数值可以增长

多层感知器(神经网络与激活函数)

输入(x1,x2,x3) 1    权重(w1,w2,w3)b    求和       传递函数    输出      

生物的神经元一层一层连接起来,当神经信号达到某一条件,这个神经元就会激活,然后传递信息下去,

为了继续使用神经网络解决这种不具备线性可分性的问题,采取在神经网络的输入端和输出端之间插入更多的神经元

输入层 ---> 隐含层 ---> 输出层 ---> 输出

神经元的启发

激活函数:

relu

sigmoid(查看之前公式)

tanh

leak relu

广告,销量的预测

逻辑回归

sigmoid函数是一个概率分布函数,给定某个输入,它将输出为一个概率值

0~1之间的一个概率值

线性回归预测的是一个连续值,

逻辑回归给出的 "是" 和 "否"的回答

逻辑回归损失函数

平方差所惩罚的是与损失为同一数量级的的情形

对于分类问题,我们最好的使用交叉熵损失函数会更有效

交叉熵会输出一个更大的 "损失"

交叉熵损失函数

交叉熵刻画的是实际输出(概率)与期望输出(概率)的距离,也就是交叉熵的值越小,两个概率分布就越接近.假设概率分布p为期望输出,概率分布q为实际输出,H(p,q)为交叉熵,


$$
H(p,q) = -\sum_{x}p(x)logq(x)
$$
在keras里,使用binary_crossentropy来计算交叉熵

Softmax分类

对数几率回归解决的是二分类的问题

对于多个选项的问题,我们可以使用softmax函数

它是对数几率回归在N个可能不同的值上的推广

神经网络的原始输出不是一个概率值,实际上只是输入的数值做了复杂的加权和与非线性处理之后的一个值而已,那么如何将这个输出变为概率分布

这就是Softmax层的作用

softmax要求每个样本必须属于某个类别,且所有可能的样本均被覆盖

softmax个样本分量之和为1

当只有两个类别时,与对数几率回归完全相同

在tf.keras里,对于多分类问题使用categorical_crossentropy和sparse_categorical_crossentopy来计算softmax交叉熵

Fashion MNIST数据集: 70000张灰度图像,10个类别  :衣物

Fashion MNIST的作用是成为经典MNIST数据集的简易替换,MNIST数据集包含手写数字(0,1,2等)的图像,这些图像的格式与本节课中使用的服饰图像的格式相同

Fashion MNIST比常规MNIST手写数据集更挑战性.

这两个数据集都相对较小.用于验证某个算法能否如期正常运行,它们都是测试和调试代码的良好起点

MNIST手写数据集将作为作业交给大家自己完成\

使用60000张图像训练网络,使用10000张图像评估经过学习的网络分类图像的准确率

可以从Tensorflow 直接访问Fashion MNIST, 只需导入和加载数据即可

优化函数,学习速率,反向传播算法

学习速率:

	梯度就是表明损失函数相对参数的变化率
	
	对梯度进行缩放的参数被称为学习速率
	
	学习速率是一种超参数或对模型的一种手工可配置的设置

 需要为它指定正确的值. 如果学习速率太小,则找到损失函数极小值点时可能需要许多迭代,如果太大,则算法可能会 "跳过" 极小值点并且因周期性的 "跳跃" 而永远无法找到极小值点

在具体实践中,可以通过查看损失函数值随时间的变化曲线,来判断学习速率的选取是合适的

合适的学习速率,损失函数会随时间下降,直到一个底部,不合适的学习速率,损失函数可能会发生震荡

在调整学习速率时,既需要使其足够小,保证不至于发生超调,也要保证它足够大,以使损失函数能够尽快下降,从而可通过较少次数的迭代更快的完成学习

反向传播算法:

	反向传播算法是一种高效计算数据流图中梯度的技术
	
	每一层的导数都是后一层的导数与前一层输出之积,这正是链式法则的奇妙之处,误差反向传播算法利用的正是这一特点

前馈时,从输入开始,逐一计算每个隐含层的输出,直到输出层

然后开始计算导数,并从输出层经各隐含层逐一反向传播,为了减少计算量,还需对所有已完成的元素进行复用,这便是反向传播算法名称的由来

常见的优化函数

优化器是编译模型所需的两个参数之一

你可以先实例化一个优化器对象,然后将它传入

model.compile(),或者你可以通过名称来调用优化器,在后一种情况下,将使用优化器的默认参数

1,SGD:随机梯度下降优化器

随机梯度下降优化器SGD和min-batch是同一个意思,抽取m个小批量(独立同分布)样本,通过计算他们平均度均值

SGD参数(了解):

lr: float >= 0.学习率

momentum:float >= 0.参数,用于加速SGD在相关方向上前进,并抑制震荡

decay:float >= 0.每次参数更新后学习率衰减值

nesterov:boolean.是否使用Nesterov动量

RMSprop: 经验上,RMSProp被证明有效且实用的深度学习网络优化算法

RMSProp增加了一个衰减系数来控制历史信息的获取多少

RMSProp会对学习率进行衰减

建议使用优化器默认参数(除了学习率lr,它可以被自由调节)

这个优化器通常是训练神经网络RNN的不错选择

2,Adam优化器(最常用,用的最多)

	1,Adam优化器可以看做是修正后的Momentum+RMSProp算法
	
	2,Adam通常被认为对超参数的选择相当棒
	
	3,学习率建议为0.001

Adam是一种可以替代传统随机梯度下降过程的一阶优化算法,它能基于训练数据迭代地更新神经网络权重

Adam通过计算梯度的一阶矩估计和二阶矩估计而为不同的参数设计独立的自适应性学习率

参数:

lr: float >= 0.学习率

beta_1:float, 0 < beta < 1.通常接近于1

beta_2:float, 0 < beta < 1.通常接近于1

decay:float >= 0.每次参数更新后学习率衰减值

网络优化与参数选择

网络容量:

	与网络中的可训练参数成正比

网络中的神经单元数越多,层数越多,神经网络的拟合能力越强

但是训练速度,难度越大,越容易产生过拟合

如何选择超参数:

	就是在搭建神经网络中,需要我们自己如选择(不是通过梯度下降算法去优化)的那些参数

eg:中间层的神经元个数,学习速率

如何提高网络的拟合能力

一种显然的想法是增大网络容量

1,增加层(这个方法比较显著)

2,增加隐藏神经元个数

	注意:单层的神经元个数,不能太小,太小的话,会造成信息瓶颈,使得模型欠拟合

Dropout:可以解决过拟合

1) 取平均的作用:  先回到标准的模型即没有dropout我们用相同的训练数据区训练5个不同的神经网络,一般会得到5个不同的结果,此时我们可以采用 '5个结果取均值' 或者 '多数取胜的投票策略' 去决定最终结果

2) 减少神经元之间复杂的共适应关系,因为dropout程序导致两个神经元不一定每次在一个dropout网络中出现,这样权值的更新不再依赖于有固定关系的隐含节点的共同作用,阻止了某些特征仅仅在其他特征下才有效果的情况

3)类似于性别在生物进化中的角色

物种为了生存往往会倾向于适应这种环境,环境突变则会导致物种难以做出及时反映,性别的出现可以繁衍出适应新环境的变种,有效的阻止过拟合,即避免环境改变时物种可能面临的灭绝

参数选择的原则:

	理想的模型是刚好在欠拟合和过拟合的界线上,也就是正好拟合数据

首先开发一个过拟合的模型:

(1)添加更多的层

(2)让每一层变得更大

(3)训练更多的轮次

然后抑制过拟合:

(1)dropout

(2)正则化

(3)图像增强

再次,调节超参数,

学习速率

隐藏层单元数

训练轮次

超参数的选择是一个经验和不断测试的结果

经典机器学习的方法,如特征工程,增加训练数据也要做

交叉验证

构建网络的总原则:	保证神经网络容量足够拟合数据

一. 增大网络数量,知道过拟合

二, 采取措施抑制过拟合

三. 继续增大网络容量,直到过拟合
